{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lots of the code was taken from our CNN assignment (to utilize the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pathlib\n",
    "import csv\n",
    "import torch.utils.tensorboard as tb\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random \n",
    "import os, math\n",
    "\n",
    "\n",
    "#STUDENT's TODO\n",
    "NOTEBOOK=1 #turn to zero before submitting\n",
    "\n",
    "# Object labels used in this programming homework \n",
    "LABEL_NAMES = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock':9 }\n",
    "\n",
    "LABEL_=['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Users\\Jeff\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:7558: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get spectrograms\n",
    "\n",
    "cmap = plt.get_cmap('magma')\n",
    "\n",
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "for g in genres:\n",
    "    pathlib.Path(f'img_data_graph/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'./genres/{g}'):\n",
    "        songname = f'./genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=5)\n",
    "        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Hz')\n",
    "        cl = plt.colorbar()\n",
    "        cl.ax.set_title('dB')\n",
    "        plt.savefig(f'img_data_graph/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n",
    "        plt.clf()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the label csv for valid and train\n",
    "header = ['file', 'label']\n",
    "file = open('labels.csv', 'w', newline='')\n",
    "\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "for g in genres:\n",
    "    for filename in os.listdir(f'img_data/valid/{g}'):\n",
    "        row = f'{filename} {g}'\n",
    "        file = open('labels.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(row.split())\n",
    "            \n",
    "for g in genres:\n",
    "    for filename in os.listdir(f'img_data/train/{g}'):\n",
    "        row = f'{filename} {g}'\n",
    "        file = open('labels.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(row.split())\n",
    "            \n",
    "# then I moved every img_data out of the genre folder (prob could have done this programmatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, image_path,data_transforms=None):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        Hint: Use the python csv library to parse labels.csv\n",
    "        \"\"\"\n",
    "        #parse csv file and initialize data_transform\n",
    "        self.image_path = image_path\n",
    "        filename = 'labels.csv'\n",
    "        csv = os.path.join(self.image_path, filename)\n",
    "        data = pd.read_csv(csv)\n",
    "        new_labels = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock':9 }\n",
    "        data.label = [new_labels[item] for item in data.label]\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.transforms = data_transforms\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        return a tuple: img, label\n",
    "        \"\"\"\n",
    "\n",
    "        image_name = os.path.join(self.image_path, self.data.iloc[idx, 0])\n",
    "        image = Image.open(image_name)\n",
    "        image_arr = np.asanyarray(image)\n",
    "        image_tensor = torchvision.transforms.ToTensor()(image_arr)\n",
    "\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        return (image_tensor, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data():\n",
    "    # Student's TODO. Replace ' ' by your path to training data folder\n",
    "    Path_to_your_data= 'img_data/train'\n",
    "    \n",
    "    dataset = MusicDataset(image_path=Path_to_your_data)\n",
    "\n",
    "    f, axes = plt.subplots(3, len(LABEL_NAMES))\n",
    "\n",
    "    counts = [0]*len(LABEL_NAMES)\n",
    "\n",
    "    for img, label in dataset:\n",
    "        c = counts[label]\n",
    "\n",
    "        if c < 3:\n",
    "            ax = axes[c][label]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "            ax.axis('off')\n",
    "            ax.set_title(LABEL_[label])\n",
    "            counts[label] += 1\n",
    "        \n",
    "        if sum(counts) >= 3 * len(LABEL_NAMES):\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "if NOTEBOOK:\n",
    "    visualize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLoss(torch.nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        Compute mean(-log(softmax(input)_label))\n",
    "        @input:  torch.Tensor((B,C)), where B = batch size, C = number of classes\n",
    "        @target: torch.Tensor((B,), dtype=torch.int64)\n",
    "        @return:  torch.Tensor((,))\n",
    "        Hint: use torch.nn.functional.nll_loss and torch.nn.functional.log_softmax\n",
    "        More details: https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.nll_loss).\n",
    "        \"\"\"\n",
    "        \n",
    "        m = torch.nn.functional.log_softmax(input)\n",
    "        nll_loss = torch.nn.functional.nll_loss(m, target)\n",
    "        return nll_loss\n",
    "\n",
    "def test_ClassificationLoss():\n",
    "    # Cross entropy loss\n",
    "    base_loss_obj    = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    # Your loss \n",
    "    student_loss_obj = ClassificationLoss()\n",
    "    \n",
    "    # Run 100 tests \n",
    "    for i in range(100):\n",
    "      dummy_logit  = torch.randn([10,6])\n",
    "      dummy_target = torch.randint(0, 6, [10])\n",
    "      student_loss = student_loss_obj(dummy_logit, dummy_target)\n",
    "      base_loss    = base_loss_obj(dummy_logit, dummy_target)\n",
    "\n",
    "      # Check type\n",
    "      if not torch.is_tensor(student_loss):\n",
    "          print(f\"[Fail!] ClassificationLoss.forward(...) must return a tensor!, but yours returns: {type(student_loss)}\")\n",
    "          return \n",
    "      # Check size\n",
    "      if student_loss.size():\n",
    "          print(f\"[Fail!] ClassificationLoss.forward(...) must return a tensor of size torch.Size([])!, but your return's size: {student_loss.size()}\")\n",
    "          return\n",
    "      # Check value\n",
    "      if torch.abs(student_loss - base_loss) > 1e-6:\n",
    "          print(f\"[Fail!] ClassificationLoss.forward(...) returned value is not correct! It should be {base_loss}, but yours returns: {student_loss}\")\n",
    "          return\n",
    "    # If you pass all, congrats!\n",
    "    print(\"[SUCCESSFUL!] Congrats! Your implementation of ClassificationLoss is correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL!\n",
    "def init_weights(net):\n",
    "    \"\"\"\n",
    "    Usage: net = Model()\n",
    "           net.apply(init_weights)\n",
    "    \"\"\"\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "#             if m.bias is not None:\n",
    "#                 stdv = 1. / math.sqrt(m.weight.size(1))\n",
    "#                 nn.init.uniform_(m.bias, -stdv, stdv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, device=torch.device('cpu'), dtypes=None, verbose=True):\n",
    "    result, params_info = summary_string(\n",
    "        model, input_size, batch_size, device, dtypes)\n",
    "    if verbose:\n",
    "        print(result)\n",
    "\n",
    "    return params_info\n",
    "\n",
    "\n",
    "def summary_string(model, input_size, batch_size=-1, device=torch.device('cpu'), dtypes=None):\n",
    "    if dtypes == None:\n",
    "        dtypes = [torch.FloatTensor]*len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "            \n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "        \"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    summary_str += line_new + \"\\n\"\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    total_conv2d = 0\n",
    "    total_linear = 0 \n",
    "    for layer in summary:\n",
    "        if 'conv2d' in layer.lower():\n",
    "            total_conv2d += 1\n",
    "        if 'linear' in layer.lower():\n",
    "            total_linear += 1 \n",
    "\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        summary_str += line_new + \"\\n\"\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(sum(input_size, ()))\n",
    "                           * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. /\n",
    "                            (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    summary_str += \"Total Conv2d layers: {0:,}\".format(total_conv2d) + \"\\n\"\n",
    "    summary_str += \"Total Linear layers: {0:,}\".format(total_linear) + \"\\n\"\n",
    "    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n",
    "    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n",
    "    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n",
    "                                                        trainable_params) + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n",
    "    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n",
    "    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n",
    "    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    # return summary\n",
    "    return summary_str, {'total_params': total_params, \n",
    "                         'total_trainable_params': trainable_params,\n",
    "                         'total_conv2d': total_conv2d,\n",
    "                         'total_linear': total_linear}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class BasicCNNClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Task: create a model with:\n",
    "          2 convolutional layers, followed by 2 linear layers, the last of which should output the logits for each class.\n",
    "          Check the table given above (section 3.3.2) for more details in the specification.\n",
    "        \"\"\"\n",
    "        # Don't remove the following line. Otherwise, it would raise ```AttributeError: cannot assign module before Module.__init__() call``` exception ERROR!\n",
    "        super(BasicCNNClassifier, self).__init__() \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "        self.linear1 = nn.Linear(128 *7*11,4428)\n",
    "        self.linear2 = nn.Linear(4428,2214)\n",
    "        self.linear = nn.Linear(2214,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        @Brief: This function takes as input a tensor x of size Bx3x64x64 \n",
    "        and outputs a \"logit\" tensor of size Bx6. Do not include a softmax layer \n",
    "        here because most of Pytorch's loss functions take \"logit\" as one of the inputs\n",
    "        while integrating log() with softmax() into a log_softmax() function.    \n",
    "        @Inputs: \n",
    "          x: torch.Tensor((B,3,64,64)) \n",
    "        @return: torch.Tensor((B,6))\n",
    "        @Note: After the 2nd Conv2d (and ReLU), the intermediate feature has the shape \n",
    "               ```B x 16 x 14 x 14```. Before putting it into layer 3 (Linear), \n",
    "               make sure to reshape this intermediate feature to ```B x 3136```.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE \n",
    "        x = self.maxpool2d(self.relu(self.conv1(x)))\n",
    "        x = self.maxpool2d(self.relu(self.conv2(x)))\n",
    "        x = self.maxpool2d(self.relu(self.conv3(x)))\n",
    "        x = self.maxpool2d(self.relu(self.conv4(x)))\n",
    "        x = self.maxpool2d(self.relu(self.conv5(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), 128 *7*11)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "def test_BasicCNNClassifier():\n",
    "    # Run randomseed here to ensure results are consistent\n",
    "    runRamdomSeed()\n",
    "    student_net = BasicCNNClassifier()\n",
    "\n",
    "    # Investigate your network's layers\n",
    "    # Compare the printed shape with what expected in the specification\n",
    "    print(\"\\n========= Model summarization ============ \") \n",
    "    student_net_info = summary(student_net, (4, 288, 432), device='cpu')\n",
    "\n",
    "  \n",
    "    # Check the number of Conv2d layers and Linear layers\n",
    "    total_conv2d = student_net_info['total_conv2d']\n",
    "    total_linear = student_net_info['total_linear']\n",
    "#     if total_conv2d != 2:\n",
    "#         print(f\"[FAIL!] BasicCNNClassifier.forward(...) must contains exactly 2 Conv2d layers!, but yours consists of: {total_conv2d} Conv2d layers\")\n",
    "#         return \n",
    "#     if total_linear != 2:\n",
    "#         print(f\"[FAIL!] BasicCNNClassifier.forward(...) must contains exactly 2 Linear layers!, but yours consists of: {total_linear} Linear layers\")\n",
    "#         return \n",
    "\n",
    "    # Check total number of parameters\n",
    "    total_parmas = student_net_info['total_params']\n",
    "#     if total_parmas != 404766:\n",
    "#         print(f\"[FAIL!] BasicCNNClassifier.forward(...) must contains exactly 404,766 parameters!, but yours consists of: {total_parmas} parameters\")\n",
    "#         print(f\"Check kernel size, stride, no of input features, no of output featurs of your Conv2d layer,\")\n",
    "#         print(f\"\\t and no of input features, no of output featurs of your Linear layer\")\n",
    "#         return \n",
    "\n",
    "    # Initialize weights for this network\n",
    "    student_net.apply(init_weights)\n",
    "\n",
    "    # Give the network a dummy input of size 2x3x64x64\n",
    "    batch_size  = 2\n",
    "    dummy_x  = torch.randn([batch_size, 4, 288, 432])\n",
    "    logit    = student_net(dummy_x) \n",
    "\n",
    "    print(\"========= Run Model with Dummy Input ============\\n\") \n",
    "#     # Check type of the output logit\n",
    "#     if not torch.is_tensor(logit):\n",
    "#         print(f\"[FAIL!] BasicCNNClassifier.forward(...) must return a tensor!, but yours returns: {type(logit)}\")\n",
    "#         return \n",
    "\n",
    "#     # Check size of the output logit\n",
    "#     if logit.size() != torch.Size([2,6]):\n",
    "#         print(f\"[FAIL!] BasicCNNClassifier.forward(...) must return a tensor of size torch.Size([2,6])!, but your return's size: {logit.size()}\")\n",
    "#         print(f\"Make sure that the number of layers, the layers, number of features, kernel size ... all are correct!\")\n",
    "#         return\n",
    "  \n",
    "    # Check value\n",
    "    expected_logit = torch.tensor([[-1.7501, -2.2915,  0.2061, -0.9353,  0.5314,  1.1240],\n",
    "                                   [-1.3631, -1.3827,  1.7212, -0.1081, -1.3976,  2.3925]])\n",
    "\n",
    "#     if torch.norm(logit - expected_logit) > 1e-3:\n",
    "#         print(f\"[FAIL!] BasicCNNClassifier.forward(...) must return \\n {expected_logit}\")\n",
    "#         print(f\"However, yours returns: \\n{logit}\")\n",
    "#         print(f\"Make sure that the number of layers, the layers, number of features, kernel size ... all are correct!\")\n",
    "#         return\n",
    "\n",
    "    # Check to see if we can do backpropagation\n",
    "    base_loss_obj  = torch.nn.CrossEntropyLoss(reduction='mean')  \n",
    "    dummy_target   = torch.randint(0, 6, [2])\n",
    "    loss           =  base_loss_obj(logit, dummy_target)\n",
    "    loss.backward()\n",
    "    print(f\"Target: {dummy_target}\")\n",
    "    print(f\"Logit: {logit}\")\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------\") \n",
    "    print(\"\\n[SUCCESSFUL!] Congrats! Your implementation of ClassificationLoss looks correct!\")\n",
    "    \n",
    "if NOTEBOOK:\n",
    "    test_BasicCNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL\n",
    "from torch import save\n",
    "from torch import load\n",
    "from os import path\n",
    "\n",
    "def check_model_exist_by_name(model_name):\n",
    "    if os.path.exists(model_name + \".pth\") and os.path.isfile(model_name + \".pth\"):\n",
    "        print(f\"[Successly trained and saved!] Your model named {model_name}.pth has been saved! DO NOT change the file's name! Just include it in your submission files.\")\n",
    "    else:\n",
    "        print(f\"[Successly trained but failed to save!] Your model named {model_name}.pth has not been saved or saved in a diffrent name from what we expect!\")\n",
    "        import glob\n",
    "        pt_files = glob.glob(\"*.pth\")\n",
    "        if pt_files:\n",
    "            print(f\"---> Somehow you've saved models as, {pt_files} which is not what autograder expects!\")\n",
    "            print(f\"---> We expect that the trained model to be saved exactly as {model_name}.pth\")\n",
    "            print(f\"---> What you can do is to manually rename the trained model to be {model_name}.pth\")\n",
    "        else:\n",
    "            print(f\"---> We found no saved models in *.pth format at all! Manually check if your saved the models in other formats or they are not saved at all!\")\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    if isinstance(model, BasicCNNClassifier) or isinstance(model, MyBestCNNClassifier):\n",
    "        return save(model.state_dict(), name + \".pth\")\n",
    "    \n",
    "    raise ValueError(\"model type '%s' not supported!\"%str(type(model)))\n",
    "\n",
    "\n",
    "def load_model(name, device_name='cpu'):\n",
    "    \"\"\"\n",
    "    @Brief: load a model saved in the \".pth\" or \".pt\" formats\n",
    "    @Inputs:\n",
    "        name (str): name of the model (without the extension)\n",
    "        device_name (str): name of the device i.e: 'cpu', 'cuda:0', that you would want to run the model on.\n",
    "    @Outputs:\n",
    "        r (nn.Module): a Pytorch model of either \"BasicCNNClassifier\" or \"MyBestCNNClassifier\" (depend on \"name\" input) \n",
    "            with pretrained wieghts.\n",
    "    \"\"\"\n",
    "    # In case students set input name = \"*.pth\" \n",
    "    if \".\" in name:\n",
    "        name = name.split('.')[0]\n",
    "        \n",
    "    if name == \"BasicCNNClassifier\":\n",
    "        r = BasicCNNClassifier()\n",
    "    elif name == \"MyBestCNNClassifier\":\n",
    "        r = MyBestCNNClassifier()\n",
    "    else:\n",
    "        raise ValueError(f\"model {name} has not been supported! Check the spelling!\")\n",
    "    r.load_state_dict(load(name + \".pth\", map_location=device_name))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL!\n",
    "def dummy_logging(train_logger, valid_logger):\n",
    "\n",
    "    \"\"\"\n",
    "    @Brief: given two Tensorboard writers (train_logger and valid_logger), write \n",
    "    some simple lines of code to dump ```dummy_train_loss``` and ```dummy_train_accuracy```\n",
    "    as well as ```dummy_valid_loss``` and ```dummy_valid_accuracy``` in each interation\n",
    "    \"\"\"\n",
    "\n",
    "    global_step = 0\n",
    "    # This is a strongly simplified training loop\n",
    "    for epoch in range(10):\n",
    "        torch.manual_seed(epoch)\n",
    "        for iteration in range(20):\n",
    "            dummy_train_loss = 0.9**(epoch+iteration/20.)\n",
    "            dummy_train_accuracy = epoch/10. + torch.randn(10)\n",
    "            global_step += 1\n",
    "            train_logger.add_scalar('loss',dummy_train_loss,global_step)\n",
    "            \n",
    "        train_logger.add_scalar('accuracy',torch.mean(dummy_train_accuracy).item(),epoch)\n",
    "     \n",
    "        torch.manual_seed(epoch)\n",
    "        for iteration in range(10):\n",
    "            dummy_validation_accuracy = epoch / 10. + torch.randn(10)\n",
    "        valid_logger.add_scalar('accuracy',torch.mean(dummy_validation_accuracy).item(),epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL!\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    outputs_idx = outputs.max(1)[1].type_as(labels)\n",
    "    return outputs_idx.eq(labels).float().mean()\n",
    "\n",
    "def predict(model, inputs, device='cpu'):\n",
    "    inputs = inputs.to(device)\n",
    "    logits = model(inputs)\n",
    "    return F.softmax(logits, -1)\n",
    "\n",
    "def draw_bar(axis, preds, labels=None):\n",
    "    y_pos = np.arange(6)\n",
    "    axis.barh(y_pos, preds, align='center', alpha=0.5)\n",
    "    axis.set_xticks(np.linspace(0, 1, 10))\n",
    "    \n",
    "    if labels:\n",
    "        axis.set_yticks(y_pos)\n",
    "        axis.set_yticklabels(labels)\n",
    "    else:\n",
    "        axis.get_yaxis().set_visible(False)\n",
    "    \n",
    "    axis.get_xaxis().set_visible(False)\n",
    "\n",
    "def visualize_predictions(model=None, model_name=None, device_name='cpu'):\n",
    "  \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = load_model(model_name, device_name)\n",
    "    \n",
    "    # Get the device \n",
    "    if device_name is not None:\n",
    "        device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "\n",
    "    validation_image_path='./img_data/valid' #enter the path \n",
    "\n",
    "    dataset = MusicDataset(image_path=validation_image_path)\n",
    "\n",
    "    f, axes = plt.subplots(2, 10)\n",
    "\n",
    "    idxes = np.random.randint(0, len(dataset), size=10)\n",
    "\n",
    "    for i, idx in enumerate(idxes):\n",
    "        img, label = dataset[idx]\n",
    "        preds = predict(model, img[None], device=device).detach().cpu().numpy()\n",
    "\n",
    "        axes[0, i].imshow(TF.to_pil_image(img))\n",
    "        axes[0, i].axis('off')\n",
    "        draw_bar(axes[1, i], preds[0], LABEL_ if i == 0 else None)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path, data_transforms=None, num_workers=0, batch_size=128):\n",
    "    dataset = MusicDataset(dataset_path,data_transforms)\n",
    "    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "  def __init__(self):\n",
    "    self.learning_rate = 0.0001\n",
    "    self.log_dir = './my_tensorboard_log_directory'\n",
    "\n",
    "args = Args();\n",
    "# Add attributes to args here, such as:\n",
    "# args.learning_rate = 0.0001\n",
    "# args.log_dir = './my_tensorboard_log_directory' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def train(args, model_name=\"MyBestCNNClassifier\"):\n",
    "    \"\"\"\n",
    "    @Brief: training your model. This should include the following items:\n",
    "        - Initialize the model (already given). Only need to map the model to the device on which you would want to run the model on \n",
    "                using the following syntax: \n",
    "                model = model.to(device) \n",
    "                where device = torch.device(<device_name>), \n",
    "                i.e: device = torch.device(\"cuda:0\") or device = torech.device(\"cpu\")\n",
    "                    \n",
    "        - Initialize tensorboard summarizers (already given)\n",
    "        - Initialize data loaders (you need to code up)\n",
    "        - Initialize the optimizer (you need to code up. Type is of your choice)\n",
    "        - Initialize the loss function (you should have coded up above)\n",
    "        - A for loop to iterate through many epochs (up to your choice). In each epoch:\n",
    "                - Iterate through every mini-batches (remember to map data and labels to the device that you would want to run the model on)\n",
    "                        - Run the forward path\n",
    "                        - Get loss\n",
    "                        - Calculate gradients \n",
    "                        - Update the model's parameters\n",
    "                - Evaluate your model on the validation set\n",
    "                - Save the model if the performance on the validation set is better using exactly the following line:\n",
    "                        save_model(model, model_name) \n",
    "                 \n",
    "    @Inputs: \n",
    "        Args: object of your choice to carry arguments that you want to use within your training function. \n",
    "    @Output: \n",
    "        No return is necessary here. \n",
    "    \"\"\"\n",
    "    # Do not touch the following lines\n",
    "    # Initialize the model \n",
    "    if model_name == \"MyBestCNNClassifier\":\n",
    "        model = MyBestCNNClassifier()\n",
    "    else:\n",
    "        model = BasicCNNClassifier()\n",
    "    # Initialize tensorboard loggers\n",
    "    if args.log_dir is not None:\n",
    "        train_logger = tb.SummaryWriter(path.join(args.log_dir, 'train'))\n",
    "        valid_logger = tb.SummaryWriter(path.join(args.log_dir, 'valid'))\n",
    "    \n",
    "    # Create subfolders to save the tensorboard log files\n",
    "    if not os.path.exists(path.join(args.log_dir, f'train/{model_name}')):\n",
    "        os.makedirs(path.join(args.log_dir, f'train/{model_name}'))\n",
    "    if not os.path.exists(path.join(args.log_dir, f'valid/{model_name}')):\n",
    "        os.makedirs(path.join(args.log_dir, f'valid/{model_name}'))  \n",
    "    #----------------------------------------\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    train_loader = load_data('img_data/train')\n",
    "    criterion = ClassificationLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    epochs = 16\n",
    "    train_loss = []\n",
    "    for epoch in range(epochs):\n",
    "      running_loss = 0\n",
    "      for itr, (image, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = model(image)\n",
    "        loss = criterion(y_predicted, label)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      train_loss.append(running_loss)\n",
    "      # if (epoch+1) % 2 == 0:\n",
    "      #   print(f'epoch: {epoch+1}, loss: {running_loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    # Don't touch the following lines\n",
    "    #You are not returning a model, but rather saving it to a file, which you will upload along with the homework4.py file\n",
    "    save_model(model, model_name) \n",
    "    # Make sure the file has been saved \n",
    "    assert os.path.exists(model_name + \".pth\") and os.path.isfile(model_name + \".pth\"), f\"[Fail to save your model named {model_name}.pth!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BasicCNNClassifier and save the model\n",
    "if NOTEBOOK:\n",
    "    model_name=\"BasicCNNClassifier\"\n",
    "    train(args, model_name=model_name)\n",
    "    \n",
    "    # Make sure that the model you've trained above has already been saved! \n",
    "    check_model_exist_by_name(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_labels(preds, labels):\n",
    "    return np.sum(preds == labels)/len(preds)\n",
    "\n",
    "def get_model_accuracy(model_name, device_name='cpu'):\n",
    "    \"\"\"\n",
    "    @Brief: evaluate the model's accuracy on the test set. In this function, instead of using the model that is already  \n",
    "            available in the current running session, we attempt to retrieve it using a Pytorch function called \n",
    "            torch.load(...). \n",
    "            This step is gonna be executed with device_name='cpu' to evaluate your code's submission on gradescope so if it fails here, your submission would likely fail.\n",
    "    @Inputs: \n",
    "        model_name (str): name of the model (without its exitension)\n",
    "        device_name (str): name of the device on which the model is run i.e: 'cpu', 'cuda:0' ...\n",
    "    \"\"\"\n",
    "    if \".\" in model_name:\n",
    "        model_name = model_name.split(\".\")[0]\n",
    "        \n",
    "    data = load_data(\"./img_data/valid\")\n",
    "    \n",
    "    device = torch.device(device_name)\n",
    "    \n",
    "    model = load_model(model_name, device_name)\n",
    "    model = model.to(device)\n",
    "    batch_size = 2\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for (X, Y) in data:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        y_pred = torch.argmax(model(X), dim = 1).tolist()\n",
    "        y_pred = map(int, y_pred)\n",
    "        preds.extend(list(y_pred))\n",
    "        labels.extend(Y.tolist())\n",
    "    return accuracy_labels(np.array(preds), np.array(labels))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the two models on the test set.\n",
    "\n",
    "def test_accuracy_models():\n",
    "    \"\"\"\n",
    "    @Brief: evaluate the accuracy of your models on the validation set. \n",
    "    @Inputs: empty \n",
    "    @Outputs: empty\n",
    "    @Note:\n",
    "        device_name (str): name of the device on which your model is run i.e: 'cpu', 'cuda:0'. To speed up, \n",
    "        change it to your cuda device (if you're running using colab or your machine has NVIDIA GPU with all \n",
    "        drivers and CUDA installed correctly). \n",
    "    For example, \n",
    "                device_name = 'cuda:0'\n",
    "\n",
    "    Before your submission, make sure to set device_name = 'cpu' and run again to make sure that your trained model\n",
    "    could be loaded and run successfully on CPU on gradescope. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Student's TODO\n",
    "    device_name = 'cpu'\n",
    "\n",
    "    b = get_model_accuracy(\"BasicCNNClassifier\", device_name=device_name)\n",
    "    print(b)\n",
    "#     if (b >= BASIC_ACC_THRESH):\n",
    "#         print(f\"*\\tBasic model is successful with accuracy {b} >= {BASIC_ACC_THRESH}\")\n",
    "#     else:\n",
    "#         print(f\"*\\tBasic model is NOT successful as accuracy {b} < {BASIC_ACC_THRESH}\")\n",
    "\n",
    "#     a = get_model_accuracy(\"MyBestCNNClassifier\", device_name=device_name)\n",
    "\n",
    "#     if (a >= BEST_ACC_THRESH):\n",
    "#         print(f\"\\n*\\tYour best model seems to be alright as it has accuracy {a} >= {BEST_ACC_THRESH}!\")\n",
    "#         print(\"*\\tKeep tuning the architecture/hyperparameters to step up on the leaderboard!\")\n",
    "#     else:\n",
    "#         print(f\"\\n*\\tYour best model seems to be not good enough to get full credit as it has accuracy {a} < {BEST_ACC_THRESH}!\")\n",
    "#         print(\"*\\tIf your model is already more complicated than the base model, check if it's converged or NOT!\")\n",
    "\n",
    "if NOTEBOOK==1 :\n",
    "    test_accuracy_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
